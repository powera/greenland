#!/usr/bin/python3

"""Generator for antonym benchmark questions."""

import logging
from typing import Optional, Iterator

from sqlalchemy.orm import Session

from lib.benchmarks.base import *
from lib.benchmarks.data_models import (
    BenchmarkQuestion, AnswerType, Difficulty, EvaluationCriteria, BenchmarkMetadata
)
from lib.benchmarks.factory import generator, benchmark

# Configure logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

# Register benchmark metadata
BENCHMARK_CODE = "0016_antonym"
BENCHMARK_NAME = "Antonym Identification"
BENCHMARK_DESCRIPTION = "Tests ability to identify the correct antonym from a list of options."

# Apply benchmark decorator to this module
benchmark(BENCHMARK_CODE, BENCHMARK_NAME, BENCHMARK_DESCRIPTION)(__name__)

@generator(BENCHMARK_CODE)
class AntonymGenerator(BenchmarkGenerator):
    """Generator for antonym benchmark questions."""
    
    def __init__(self, metadata: BenchmarkMetadata, session: Optional[Session] = None):
        super().__init__(metadata, session)
        
        # Configure generation strategies
        self.can_load_from_file = True
        self.can_generate_locally = False
        self.can_generate_with_llm = True
        
        # Set file paths
        self.questions_directory = None
        
        # Set LLM context
        self.context = """You are a linguistics assistant. Generate challenging antonym questions that:
1. Include a target word
2. Provide 6 candidate words, only one of which is a true antonym
3. Ensure the other 5 candidates are plausible distractors (synonyms, related words, etc.)
4. Include a mix of easy, medium, and hard difficulty levels"""

    def _generate_from_file(self, **kwargs) -> Iterator[BenchmarkQuestion]:
        """
        Generate questions from files.
        
        Yields:
            BenchmarkQuestion objects loaded from JSON files
        """
        categories = ["adjectives", "verbs"]
        
        for category in categories:
            try:
                # Load data from JSON files
                questions_data = self.load_json_file(f"{category}.json")
                
                for item in questions_data:
                    question = BenchmarkQuestion(
                        question_text=f"What is the antonym of '{item['word']}' among these candidates: {', '.join(item['candidates'])}",
                        answer_type=AnswerType.JSON,
                        correct_answer={"antonym": item["antonym"]},
                        category=item.get("category", category),
                        difficulty=Difficulty(item.get("difficulty", "medium")),
                        choices=item["candidates"],
                        schema={
                            "type": "object",
                            "properties": {
                                "antonym": {"type": "string"}
                            },
                            "required": ["antonym"]
                        },
                        evaluation_criteria=EvaluationCriteria(
                            case_sensitive=False,
                            exact_match=True
                        )
                    )
                    yield question
                
                logger.info(f"Loaded questions from {category}.json")
            except FileNotFoundError:
                logger.warning(f"File {category}.json not found.")
            except Exception as e:
                logger.error(f"Error processing {category}.json: {str(e)}")

    def _generate_with_llm(self, **kwargs) -> Iterator[BenchmarkQuestion]:
        """
        Generate questions using a language model.
        
        Args:
            **kwargs: May include 'category' and 'difficulty' parameters
            
        Yields:
            BenchmarkQuestion objects generated by an LLM
        """
        # Get category and difficulty from kwargs or use defaults
        category = kwargs.get("category", "general")
        difficulty = kwargs.get("difficulty", "medium")
        
        # Define schema for question generation
        schema = {
            "type": "object",
            "properties": {
                "word": {"type": "string"},
                "candidates": {
                    "type": "array",
                    "items": {"type": "string"},
                    "minItems": 6,
                    "maxItems": 6
                },
                "antonym": {"type": "string"},
                "difficulty": {
                    "type": "string",
                    "enum": ["easy", "medium", "hard"]
                },
                "category": {"type": "string"}
            },
            "required": ["word", "candidates", "antonym"]
        }
        
        # Categories and difficulties to cycle through
        categories = ["adjectives", "verbs", "nouns", "adverbs"]
        difficulties = ["easy", "medium", "hard"]
        
        # Generate an unlimited number of questions
        category_index = 0
        difficulty_index = 0
        
        while True:
            current_category = kwargs.get("category", categories[category_index])
            current_difficulty = kwargs.get("difficulty", difficulties[difficulty_index])
            
            prompt = f"""Generate a single antonym question for a {current_category} word at {current_difficulty} difficulty level.
The question should have:
1. A target word
2. 6 possible answers (candidates), only one of which is a true antonym
3. The correct antonym identified within the candidates list

Return as a JSON object with 'word', 'candidates', 'antonym', 'difficulty', and 'category' fields."""

            try:
                # Use the LLM to generate question content
                item = self.get_llm_question(prompt=prompt, schema=schema)
                
                # Convert to BenchmarkQuestion format
                question = BenchmarkQuestion(
                    question_text=f"What is the antonym of '{item['word']}' among these candidates: {', '.join(item['candidates'])}",
                    answer_type=AnswerType.JSON,
                    correct_answer={"antonym": item["antonym"]},
                    category=item.get("category", current_category),
                    difficulty=Difficulty(item.get("difficulty", current_difficulty)),
                    choices=item["candidates"],
                    schema={
                        "type": "object",
                        "properties": {
                            "antonym": {"type": "string"}
                        },
                        "required": ["antonym"]
                    },
                    evaluation_criteria=EvaluationCriteria(
                        case_sensitive=False,
                        exact_match=True
                    )
                )
                
                yield question
                
                # Update indices for next iteration
                category_index = (category_index + 1) % len(categories)
                if category_index == 0:
                    difficulty_index = (difficulty_index + 1) % len(difficulties)
                    
            except Exception as e:
                logger.error(f"Error generating question: {str(e)}")
                # Continue to the next iteration rather than stopping
                continue