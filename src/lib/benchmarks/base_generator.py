#!/usr/bin/python3

"""Base classes for implementing language model benchmarks."""

import itertools
import json
import logging
import os
import uuid
import random
from typing import Dict, List, Optional, Any, Union, Tuple, Set, Iterator

import datastore.benchmarks
import constants
from clients import unified_client, ollama_client
from clients.ollama_client import OllamaTimeoutError
from lib.benchmarks.data_models import (
    BenchmarkQuestion, BenchmarkResult, BenchmarkMetadata,
    AnswerType, Difficulty, EvaluationCriteria
)
import lib.score_table
import lib.validation

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Default model for validation and LLM-based question generation
DEFAULT_VALIDATION_MODEL = "gpt-4o-mini-2024-07-18"
DEFAULT_GENERATION_MODEL = "gemma2:9b"

class BenchmarkGenerator:
    """
    Base class for generating benchmark questions.
    
    This implementation supports three different generation strategies:
    1. Load from file - questions are loaded from JSON files
    2. Generate locally - questions are generated algorithmically using local data
    3. Generate by LLM - questions are generated by prompting a language model
    
    Subclasses can configure which strategies are available by setting the
    corresponding class attributes.
    
    The class includes an iterator-based approach that automatically switches between
    strategies when one source is exhausted, ensuring efficient question generation.
    """
    
    def __init__(self, metadata: BenchmarkMetadata, session=None, auto_validate: bool = False):
        """
        Initialize generator with benchmark metadata.
        
        Args:
            metadata: Benchmark metadata
            session: Optional database session
        """
        self.metadata = metadata
        self.session = session or datastore.benchmarks.create_dev_session()
        self.auto_validate = auto_validate
        self.validation_model = DEFAULT_VALIDATION_MODEL
        
        # Strategy configuration flags (subclasses should override these)
        self.can_load_from_file = False  # Set to True if questions can be loaded from files
        self.can_generate_locally = False  # Set to True if questions can be generated locally
        self.can_generate_with_llm = False  # Set to True if questions can be generated by LLM
        
        # File paths for file-based generation (subclasses should set if applicable)
        self.questions_file_path = None  # Path to JSON file with questions
        self.questions_directory = None  # Directory containing multiple question files
        
        # Default context for LLM-based generation
        self.context = "You are a helpful assistant creating benchmark questions."
        
        # Preferred generation order (can be overridden by subclasses)
        self.strategy_order = ["file", "local", "llm"]
        
        # Cached generators to maintain state between calls
        self._file_generator = None
        self._local_generator = None
        self._llm_generator = None
        self._combined_generator = None
        
        # Ensure benchmark exists in database
        success, msg = datastore.benchmarks.insert_benchmark(
            self.session,
            codename=self.metadata.code,
            displayname=self.metadata.name,
            description=self.metadata.description or self.__doc__
        )
        if not success and "already exists" not in msg:
            logger.error("Failed to create benchmark: %s", msg)

    def load_json_file(self, filename: str, subdir: Optional[str] = None) -> Any:
        """
        Load data from a JSON file.
        
        Args:
            filename: Name of the JSON file to load
            subdir: Optional subdirectory within the benchmark directory
            
        Returns:
            Parsed JSON data
            
        Raises:
            FileNotFoundError: If file doesn't exist
            JSONDecodeError: If file contains invalid JSON
        """
        # Determine file path
        if subdir:
            file_path = os.path.join(constants.BENCHMARK_DATA_DIR, self.metadata.code, subdir, filename)
        else:
            file_path = os.path.join(constants.BENCHMARK_DATA_DIR, self.metadata.code, filename)
        
        # Load and parse the file
        try:
            with open(file_path, 'r') as f:
                data = json.load(f)
            logger.debug(f"Loaded data from {file_path}")
            return data
        except FileNotFoundError:
            logger.error(f"File not found: {file_path}")
            raise
        except json.JSONDecodeError as e:
            logger.error(f"Invalid JSON in {file_path}: {e}")
            raise

    def load_text_file(self, filename: str, subdir: Optional[str] = None) -> List[str]:
        """
        Load lines from a text file.
        
        Args:
            filename: Name of the text file to load
            subdir: Optional subdirectory within the benchmark directory
            
        Returns:
            List of non-empty lines from the file
            
        Raises:
            FileNotFoundError: If file doesn't exist
        """
        # Determine file path
        if subdir:
            file_path = os.path.join(constants.BENCHMARK_DATA_DIR, self.metadata.code, subdir, filename)
        else:
            file_path = os.path.join(constants.BENCHMARK_DATA_DIR, self.metadata.code, filename)
        
        # Load the file
        try:
            with open(file_path, 'r') as f:
                lines = [line.strip() for line in f if line.strip()]
            logger.debug(f"Loaded {len(lines)} lines from {file_path}")
            return lines
        except FileNotFoundError:
            logger.error(f"File not found: {file_path}")
            raise

    def _generate_from_file(self, **kwargs) -> Iterator[BenchmarkQuestion]:
        """
        Generate questions from files.
        
        This is a generator function that yields BenchmarkQuestion objects
        loaded from files one at a time.
        
        Args:
            **kwargs: Additional parameters to control generation
            
        Yields:
            BenchmarkQuestion objects
            
        Note:
            Subclasses should override this method if they support file-based question loading.
        """
        if not self.can_load_from_file:
            logger.debug("File-based generation is not enabled for this benchmark")
            return
            
        # This is just a placeholder. Subclasses should implement their own logic.
        logger.warning("File loading is enabled but not implemented in the subclass")
        
        # Example implementation for subclasses (commented out):
        # if self.questions_file_path:
        #     try:
        #         questions_data = self.load_json_file(self.questions_file_path)
        #         for item in questions_data:
        #             question = BenchmarkQuestion(
        #                 question_text=item['question'],
        #                 answer_type=AnswerType(item['type']),
        #                 correct_answer=item['answer']
        #             )
        #             yield question
        #     except Exception as e:
        #         logger.error(f"Error loading questions from file: {e}")
        # elif self.questions_directory:
        #     for filename in os.listdir(self.questions_directory):
        #         if filename.endswith('.json'):
        #             try:
        #                 file_path = os.path.join(self.questions_directory, filename)
        #                 with open(file_path, 'r') as f:
        #                     data = json.load(f)
        #                     question = BenchmarkQuestion(
        #                         question_text=data['question'],
        #                         answer_type=AnswerType(data['type']),
        #                         correct_answer=data['answer']
        #                     )
        #                     yield question
        #             except Exception as e:
        #                 logger.error(f"Error loading question from {filename}: {e}")
        
        # Important: Do not yield anything in the base class implementation

    def _generate_locally(self, **kwargs) -> Iterator[BenchmarkQuestion]:
        """
        Generate questions using local algorithms or data, but never LLM calls.
        
        This is a generator function that yields BenchmarkQuestion objects
        generated locally one at a time.
        
        Args:
            **kwargs: Additional parameters to control generation
            
        Yields:
            BenchmarkQuestion objects
            
        Note:
            Subclasses should override this method if they support local question generation.
        """
        if not self.can_generate_locally:
            logger.debug("Local generation is not enabled for this benchmark")
            return
            
        # This is just a placeholder. Subclasses should implement their own logic.
        logger.warning("Local generation is enabled but not implemented in the subclass")
        
        # Important: Do not yield anything in the base class implementation

    def _generate_with_llm(self, **kwargs) -> Iterator[BenchmarkQuestion]:
        """
        Generate questions using a language model.
        
        This is a generator function that yields BenchmarkQuestion objects
        generated by an LLM one at a time.
        
        Args:
            **kwargs: Additional parameters to control generation
            
        Yields:
            BenchmarkQuestion objects
            
        Note:
            Subclasses should override this method if they support LLM-based question generation.
        """
        if not self.can_generate_with_llm:
            logger.debug("LLM-based generation is not enabled for this benchmark")
            return
            
        # This is just a placeholder. Subclasses should implement their own logic.
        logger.warning("LLM generation is enabled but not implemented in the subclass")
        
        # Important: Do not yield anything in the base class implementation

    def get_llm_question(
        self,
        prompt: str,
        model: str = DEFAULT_GENERATION_MODEL,
        schema: Optional[Dict] = None,
        context: Optional[str] = None
    ) -> Any:
        """
        Generate question content using an LLM.
        
        Args:
            prompt: Prompt for the LLM
            model: Model to use for generation
            schema: Optional JSON schema for structured output
            context: Optional context to override self.context
            
        Returns:
            Response text or structured data based on schema
        """
        # Use custom context if provided, otherwise use default
        ctx = context or self.context
        
        # Generate response
        response = unified_client.generate_chat(
            prompt=prompt,
            model=model,
            json_schema=schema,
            context=ctx
        )
        
        # Return structured data if schema was provided, otherwise text
        if schema:
            return response.structured_data
        else:
            return response.response_text.strip()

    def generate_questions_iter(self, count: Optional[int] = None) -> Iterator[BenchmarkQuestion]:
        """
        Generate questions using all available strategies in the configured order.
        
        This is a generator function that yields BenchmarkQuestion objects
        one at a time, automatically switching between strategies when one
        source is exhausted.
        
        Args:
            count: Optional maximum number of questions to generate
            
        Yields:
            BenchmarkQuestion objects
        """
            
        # Otherwise, create a new combined generator
        questions_generated = 0
        
        # Try each strategy in order
        for strategy in self.strategy_order:
            # Stop if we've reached the requested count
            if count is not None and questions_generated >= count:
                break
                
            if strategy == "file" and self.can_load_from_file:
                # Initialize file generator if not already done
                if self._file_generator is None:
                    logger.info("Initializing file-based generation strategy")
                    self._file_generator = self._generate_from_file()
                
                # Use the file generator
                logger.info("Using file-based generation strategy")
                try:
                    while True:
                        question = next(self._file_generator)
                        yield question
                        questions_generated += 1
                        if count is not None and questions_generated >= count:
                            break
                except StopIteration:
                    logger.info("File-based generation exhausted")
                        
            if count is not None and questions_generated >= count:
                break
                
            if strategy == "local" and self.can_generate_locally:
                # Initialize local generator if not already done
                if self._local_generator is None:
                    logger.info("Initializing local generation strategy")
                    self._local_generator = self._generate_locally()
                
                # Use the local generator
                logger.info("Using local generation strategy")
                try:
                    while True:
                        question = next(self._local_generator)
                        yield question
                        questions_generated += 1
                        if count is not None and questions_generated >= count:
                            break
                except StopIteration:
                    logger.info("Local generation exhausted")
                        
            if count is not None and questions_generated >= count:
                break
                
            if strategy == "llm" and self.can_generate_with_llm:
                # Initialize LLM generator if not already done
                if self._llm_generator is None:
                    logger.info("Initializing LLM-based generation strategy")
                    self._llm_generator = self._generate_with_llm()
                
                # Use the LLM generator
                logger.info("Using LLM-based generation strategy")
                try:
                    while True:
                        question = next(self._llm_generator)
                        yield question
                        questions_generated += 1
                        if count is not None and questions_generated >= count:
                            break
                except StopIteration:
                    logger.info("LLM-based generation exhausted")

    def generate_question(self, strategy: Optional[str] = None, **kwargs) -> BenchmarkQuestion:
        """
        Generate a single benchmark question using the configured strategies.
        
        This method uses the generator methods but stops after getting the first question.
        
        Args:
            strategy: Optional strategy to use ("file", "local", "llm") or None to try all
            **kwargs: Additional parameters to control generation
            
        Returns:
            BenchmarkQuestion object
            
        Raises:
            ValueError: If no generation strategy succeeds
        """
        if strategy:
            # Use the specified strategy
            if strategy == "file":
                if self._file_generator is None:
                    self._file_generator = self._generate_from_file(**kwargs)
                
                try:
                    return next(self._file_generator)
                except (StopIteration, TypeError):
                    # Reset the generator for next time
                    self._file_generator = None
                    raise ValueError("File-based generation failed or exhausted")
            elif strategy == "local":
                if self._local_generator is None:
                    self._local_generator = self._generate_locally(**kwargs)
                
                try:
                    return next(self._local_generator)
                except (StopIteration, TypeError):
                    # Reset the generator for next time
                    self._local_generator = None
                    raise ValueError("Local generation failed or exhausted")
            elif strategy == "llm":
                if self._llm_generator is None:
                    self._llm_generator = self._generate_with_llm(**kwargs)
                
                try:
                    return next(self._llm_generator)
                except (StopIteration, TypeError):
                    # Reset the generator for next time
                    self._llm_generator = None
                    raise ValueError("LLM-based generation failed or exhausted")
            else:
                raise ValueError(f"Unknown generation strategy: {strategy}")
        else:
            # If we already have a combined generator, use it
            if self._combined_generator is None:
                self._combined_generator = self.generate_questions_iter()
            
            try:
                return next(self._combined_generator)
            except StopIteration:
                # Reset all generators if we've exhausted everything
                self._file_generator = None
                self._local_generator = None
                self._llm_generator = None
                self._combined_generator = None
                
                # If we get here, all strategies failed
                enabled_strategies = []
                if self.can_load_from_file:
                    enabled_strategies.append("file")
                if self.can_generate_locally:
                    enabled_strategies.append("local")
                if self.can_generate_with_llm:
                    enabled_strategies.append("llm")
                    
                if not enabled_strategies:
                    raise ValueError("No generation strategies are enabled in this generator")
                else:
                    raise ValueError(f"All enabled generation strategies are exhausted: {', '.join(enabled_strategies)}")

    def generate_validated_question(
        self,
        strategy: Optional[str] = None,
        validation_model: str = DEFAULT_VALIDATION_MODEL,
        max_attempts: int = 3,
        **kwargs
    ) -> BenchmarkQuestion:
        """
        Generate a question and validate it, retrying if needed.
        
        Args:
            strategy: Optional strategy to use ("file", "local", "llm") or None to try all
            validation_model: Model to use for validation
            max_attempts: Maximum generation attempts
            **kwargs: Passed to generate_question
            
        Returns:
            Validated BenchmarkQuestion object
            
        Raises:
            ValueError: If unable to generate a valid question after max attempts
        """
        for attempt in range(max_attempts):
            # Generate candidate question
            try:
                question = self.generate_question(strategy, **kwargs)
                
                # Validate the question
                is_valid, reason = lib.validation.validate_question(
                    question, 
                    model=validation_model
                )
                
                if is_valid:
                    logger.debug(f"Question validated successfully on attempt {attempt + 1}")
                    return question
                else:
                    logger.info(f"Attempt {attempt + 1} failed validation: {reason}")
            except Exception as e:
                logger.error(f"Error generating question on attempt {attempt + 1}: {e}")
        
        # If we get here, all attempts failed
        raise ValueError(f"Failed to generate valid question after {max_attempts} attempts")

    def save_question(self, question: BenchmarkQuestion, custom_id: Optional[str] = None) -> str:
        """
        Save generated question to database with standardized ID format.
        
        Args:
            question: BenchmarkQuestion object
            custom_id: Optional custom ID suffix
            
        Returns:
            Generated question ID
        """
        # Generate standardized question ID
        if custom_id:
            question_id = f"{self.metadata.code}:{custom_id}"
        else:
            # Generate a unique identifier based on content
            unique_part = str(uuid.uuid4())[:8]
            question_id = f"{self.metadata.code}:{unique_part}"
        
        # Convert question to dictionary and save
        question_dict = question.to_dict()
        datastore.benchmarks.insert_question(
            self.session,
            question_id,
            self.metadata.code,
            json.dumps(question_dict)
        )
        
        return question_id
        
    def batch_save_questions(self, questions: List[BenchmarkQuestion], 
                            id_prefix: Optional[str] = None) -> List[str]:
        """
        Save multiple questions with sequential IDs.
        
        Args:
            questions: List of BenchmarkQuestion objects
            id_prefix: Optional prefix for the ID sequence
            
        Returns:
            List of generated question IDs
        """
        question_ids = []
        for idx, question in enumerate(questions):
            if id_prefix:
                custom_id = f"{id_prefix}:{idx}"
            else:
                custom_id = str(idx)
                
            question_id = self.save_question(question, custom_id)
            question_ids.append(question_id)
            
        return question_ids
    
    def load_existing_questions(self) -> List[Dict]:
        """
        Load existing questions for this benchmark from the database.
        
        Returns:
            List of question dictionaries
        """
        return datastore.benchmarks.load_all_questions_for_benchmark(
            self.session, self.metadata.code
        )
    
    def clear_existing_questions(self) -> None:
        """
        Clear all existing questions for this benchmark from the database.
        This is useful when regenerating all questions from scratch.
        
        Note: This is a destructive operation!
        """
        # This method requires additional database functionality.
        # For now, we'll just log a warning.
        logger.warning(
            f"clear_existing_questions not implemented yet. " 
            f"You need to manually delete questions for benchmark {self.metadata.code}."
        )
    
    def load_to_database(self, num_questions: int = 40, strategy: Optional[str] = None) -> List[str]:
        """
        Load questions into database. The default implementation generates
        questions using generate_question() and saves them.
        
        Args:
            num_questions: Number of questions to generate
            strategy: Optional strategy to use ("file", "local", "llm") or None to try all
            
        Returns:
            List of question IDs
            
        Note: Subclasses can override this for custom loading behavior.
        """
        logger.info(f"Generating {num_questions} questions for benchmark {self.metadata.code}")
        
        # If a specific strategy is requested, use that
        if strategy:
            if strategy == "file" and not self.can_load_from_file:
                raise ValueError("File-based generation is not enabled for this benchmark")
            if strategy == "local" and not self.can_generate_locally:
                raise ValueError("Local generation is not enabled for this benchmark")
            if strategy == "llm" and not self.can_generate_with_llm:
                raise ValueError("LLM-based generation is not enabled for this benchmark")
                
            # Create a fresh generator for this strategy to ensure we start from the beginning
            if strategy == "file":
                self._file_generator = self._generate_from_file()
                question_gen = self._file_generator
            elif strategy == "local":
                self._local_generator = self._generate_locally()
                question_gen = self._local_generator
            elif strategy == "llm":
                self._llm_generator = self._generate_with_llm()
                question_gen = self._llm_generator
            else:
                raise ValueError(f"Unknown generation strategy: {strategy}")
                
            questions = []
            for i, question in enumerate(itertools.islice(question_gen, num_questions)):
                questions.append(question)
                logger.info(f"Generated question {i+1}/{num_questions} using {strategy} strategy")
        else:
            # Otherwise, create a fresh combined generator
            self._file_generator = None
            self._local_generator = None
            self._llm_generator = None
            self._combined_generator = None
            
            # Use the auto-switching iterator
            questions = list(itertools.islice(self.generate_questions_iter(num_questions), num_questions))
        
        # Save questions with batch ID
        question_ids = self.batch_save_questions(questions)
        
        logger.info(f"Successfully saved {len(question_ids)} questions to database")
        return question_ids